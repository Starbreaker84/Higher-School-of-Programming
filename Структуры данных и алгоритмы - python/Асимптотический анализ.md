## Асимптотический анализ

**Алгоритм** -- это набор инструкций, описывающих порядок действий исполнителя для достижения некоторого результата. Порядок действий может быть последовательным, может быть параллельным, и в общем случае сам по себе порядок может определяться весьма сложными правилами. Но на практике мы пользуемся простыми последовательными алгоритмами. Например, множество инженерно-технических инструкций представляют собой пошаговые руководства, по сути алгоритмы действий.

Пример алгоритма проверки, является ли число N простым (делится только на себя и на единицу):

1. Примем X равно двум.
2. Если N делится на X нацело, без остатка, то перейти к п. 7.
3. Увеличить X на единицу.
4. Если X меньше или равно N / 2 , перейти к п. 2.
5. Сообщить, что N -- простое.
6. Закончить работу.
7. Сообщить, что N -- не простое.
8. Закончить работу.

**Вычислительная (математическая) модель** -- это множество (набор) формальных операций, с помощью которых выполняется некоторое вычисление. В примере с алгоритмом мы используем набор операций "примем, что X равно ...", "увеличим X на ...", "если некоторое условие истинно, то перейти к пункту ...", "закончить работу", "сообщить что ...". В понятие вычислительной модели также входят абстрактные схемы хранения информации.

Вычислительная модель -- очень важное понятие, потому что любые оценки сложности алгоритмов, которые обычно меряются в некоторых условных "штуках" (количество сравнений, количество обращений к памяти, ...), напрямую зависят от конкретной модели.

---

Сделаем небольшое отступление в теорию. Зачем нам надо хорошо знать алгоритмы и структуры данных? Любой массовый язык предлагает развитые наборы типов данных, включающие и списки, и стеки, и очереди, и ещё множество самых разных структур. Но понимаем ли мы, как они устроены изнутри?

А зачем нам это надо? Например, если нам надо отсортировать массив из 10 элементов, достаточно двух вложенных циклов, затраты процессорного времени будут всё равно минимальны. Но если в массиве будет тысяча элементов, или миллиард, сколько времени он будет работать -- минуту или сто лет? Если клиент хочет высокой масштабируемости, как нам оценить возможности нашей системы?

Сложность алгоритма можно измерить математически. В частности, так называемое О-большое (верхняя оценка) -- O(f(n)), оценивает поведение функции f при увеличении n.
Асимптотика -- это характер изменения функции, когда n стремится к определённой точке, в качестве которой в анализе алгоритмов принимается бесконечность. O-большое на практике считается как наихудшая оценка работы алгоритма (когда данные поступают наиболее неудобным способом, заставляя алгоритм работать наиболее неэффективно).

Часто используется и о-малое -- это, условно говоря, средняя оценка, когда входные данные случайны.

Существует также теоретико-информационная нижняя оценка сложности (омега) -- это по сути идеальная ситуация, когда мы получаем максимальную скорость работы алгоритма, которую нельзя превысить даже теоретически. Например, если алгоритму сортировки подаётся на вход уже отсортированный массив, он в идеале должен демонстрировать линейную сложность, хотя на практике конкретные алгоритмы существенно различаются по данному критерию.

И четвёртое понятие в этой группе -- тета (T), которая подразумевает точную оценку сложности. Тета часто совпадает с O.

*Надо отметить, что единой терминологии по поводу оценок сложности пока не выработано, поэтому иногда в литературе возникает путаница: под нижней оценкой понимают наихудшую, а под верхней -- наилучшую.*

Вот классические меры сложности, которые желательно учитывать при выборе подходящего алгоритма и структур данных.

1. O(1) -- никак не зависит от n, то есть время вычисления всегда фиксировано независимо от объёма входных данных. Как такое может быть? Например, мы можем быстро получить длину динамического массива, которая нередко выполняется за одну операцию вне зависимости от количества объектов в нём.
2. O(n) -- линейно зависит от объёма данных. Например, расчёт длины связанного списка линейно зависит от количества элементов в нём.
3. O(log n) -- логарифмический рост сложности, обычно по основанию 2. Имеется в виду, что чем больше данных, тем медленнее растёт сложность (время выполнения). Это например, алгоритмы, которые осуществляют бинарный поиск.
4. O(n * log n) -- растёт быстрее, чем O(n). Это часто разные алгоритмы сортировки.
5. Далее идут квадратичные и иные зависимости -- O(n*n) и т. д., которые уже растут экспоненциально (очень быстро при росте наших входных данных), и если в нашей системе выявлена подобная мера, практически всегда это означает, что алгоритм надо переделывать.

Вот достаточно подробный анализ сложности разных алгоритмов:
[https://habr.com/post/188010/](https://habr.com/post/188010/)

Простая идея, что если требуется цикл, то это O(n), если вложенные циклы, то это O(n*n), и т. д.
Если можем за одно действие вычислить, например получить элемент по индексу, это O(1).
O(n*n) -- это уже плохо, желательно переделывать.
То есть как только добавляется вложенный цикл, это сигнал, что что-то пошло не так. Обычно считается удовлетворительным O(n * log n).

Например, долгое время считалось, что классическое умножение целых n-значных чисел быстрее всего выполнять "в столбик" -- его сложность O(n^2), и даже великий Андрей Колмогоров полагал, что быстрее алгоритма не существует.
Однако Анатолий Карацуба разработал алгоритм, работавший за O(n^1.5849...).

---

## Спускаемся на фундамент Python

Перед тем, как мы начнём реализовывать следующие структуры данных, познакомимся со стандартной питоновской библиотекой ctypes. Множество стандартных функций Python в целях быстродействия написаны на языке Си, и ctypes предоставляет собой своеобразный интерфейс между Python и Си (и по большому счёту, между Python и операционной системой). Нас прежде всего интересует физический доступ к объектам типа PyObject -- это самый базовый, универсальный класс Python, от которого наследуются все остальные его типы.

Обращение ctypes.py_object представляет собой так называемый указатель или ссылку, в терминологии Си, на объект PyObject. Формально в Python нельзя объявить PyObject как таковой, но вся работа с объектами Python осуществляется через указатели на этот тип, а именно через тип ctypes.py_object.

Пример: мы хотим сформировать блок оперативной памяти, где собираемся хранить последовательность значений (объектов). Для этого надо выполнить команду

(N * ctypes.py_object)()

которая отведёт в памяти N ячеек, предназначенных для хранения объектов (точнее, ссылок на объекты) Python.

```
import ctypes
A = (3 * ctypes.py_object)()
```

Теперь мы можем обращаться к A как к адресуемой области памяти! Фактически, Python разрешает нам индексировать эту область напрямую:

```
A[0] = 1024
A[1] = '1024'
A[2] = 10.24
```

Более того, A поддерживает перечисление, то есть допускается такая запись:

```
for i in A:
    print(i)
```

Очень важно, что мера сложности такой индексации -- O(1). Система времени выполнения одной операцией определяет местонахождения нужной ссылки на объект в памяти, так как все они имеют одинаковый размер (указатель на PyObject). Почему это важно, рассмотрим в следующем занятии.

**Задание.**
Попробуйте оценить какие-нибудь свои или чужие программы, которые выполняют объёмную вычислительную работу -- какова мера их сложности, как сильно растёт время их работы по мере увеличения входных данных? Напишите небольшой отчёт.
